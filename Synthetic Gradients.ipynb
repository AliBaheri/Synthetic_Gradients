{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://iamtrask.github.io/2017/03/21/synthetic-gradients/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part - 1\n",
    "\n",
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples = 1000\n",
    "output_dim = 12\n",
    "#iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int2vec(x,dim=output_dim):\n",
    "    \n",
    "    out = np.zeros(dim) # set output vector = dim number of 0s\n",
    "\n",
    "    binrep = np.array(list(np.binary_repr(x))).astype('int') # get binary representation of the integer\n",
    "\n",
    "    out[-len(binrep):] = binrep # set the last k values to binrep\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset(output_dim = 8,num_examples=1000):\n",
    "    \n",
    "    # np.random.rand(num_examples) = randomly pick num_examples number of real numbers between (0,1)\n",
    "    # multiple each by pow(2,(output_dim - 1)) and get their integer part\n",
    "\n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "    \n",
    "    # Create 2 set of numbers and third set of numbers be their sum \n",
    "    \n",
    "    \n",
    "    x = list()\n",
    "    for i in range(len(x_left_int)):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "        #just concatenate the binary of 2 integers\n",
    "\n",
    "    y = list()\n",
    "    for i in range(len(y_int)):\n",
    "        y.append(int2vec(y_int[i]))\n",
    "        #get binary of sum\n",
    "\n",
    "    x = np.array(x) # of length = 2 * output_dim\n",
    "    y = np.array(y) # of length = output_dim\n",
    "    \n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: two concatenated binary values:\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  1.  0.  1.\n",
      "  0.  1.  1.  0.  0.  0.]\n",
      "24\n",
      "\n",
      "Output: binary value of their sum:\n",
      "[ 0.  0.  1.  1.  0.  1.  1.  1.  0.  0.  0.  1.]\n",
      "12\n",
      "\n",
      "\n",
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "print(\"Input: two concatenated binary values:\")\n",
    "print(x[0])\n",
    "print len(x[0])\n",
    "print(\"\\nOutput: binary value of their sum:\")\n",
    "print(y[0])\n",
    "print len(y[0])\n",
    "\n",
    "print \"\\n\\n\", len(x), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_out2deriv(out):\n",
    "    return out * (1 - out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sythetic Gredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNI(object):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv,alpha = 0.1):\n",
    "        \n",
    "        # same as before\n",
    "        self.weights = (np.random.randn(input_dim, output_dim) * 0.2) - 0.1\n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_deriv = nonlin_deriv\n",
    "\n",
    "\n",
    "        # new stuff\n",
    "        self.weights_synthetic_grads = (np.random.randn(output_dim,output_dim) * 0.2) - 0.1\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    # used to be just \"forward\", but now we update during the forward pass using Synthetic Gradients :)\n",
    "    def forward_and_synthetic_update(self,input):\n",
    "\n",
    "    \t# cache input\n",
    "        self.input = input\n",
    "\n",
    "        # forward propagate\n",
    "        self.output = self.nonlin(self.input.dot(self.weights))\n",
    "        \n",
    "        # generate synthetic gradient via simple linear transformation\n",
    "        self.synthetic_gradient = self.output.dot(self.weights_synthetic_grads)\n",
    "\n",
    "        # update our regular weights using synthetic gradient\n",
    "        self.weight_synthetic_gradient = self.synthetic_gradient * self.nonlin_deriv(self.output)\n",
    "        self.weights += self.input.T.dot(self.weight_synthetic_gradient) * self.alpha\n",
    "        \n",
    "        # return backpropagated synthetic gradient (this is like the output of \"backprop\" method from the Layer class)\n",
    "        # also return forward propagated output (feels weird i know... )\n",
    "        return self.weight_synthetic_gradient.dot(self.weights.T), self.output\n",
    "    \n",
    "    def normal_update(self,true_gradient):\n",
    "        grad = true_gradient * self.nonlin_deriv(self.output)\n",
    "        \n",
    "        self.weights -= self.input.T.dot(grad) * self.alpha\n",
    "        self.bias -= np.average(grad,axis=0) * self.alpha\n",
    "        \n",
    "        return grad.dot(self.weights.T)\n",
    "    \n",
    "    # this is just like the \"update\" method from before... except it operates on the synthetic weights\n",
    "    def update_synthetic_weights(self,true_gradient):\n",
    "        self.synthetic_gradient_delta = self.synthetic_gradient - true_gradient \n",
    "        self.weights_synthetic_grads += self.output.T.dot(self.synthetic_gradient_delta) * self.alpha\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "num_examples = 100\n",
    "output_dim = 8\n",
    "iterations = 100000\n",
    "\n",
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "\n",
    "batch_size = 10\n",
    "alpha = 0.01\n",
    "\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 64\n",
    "layer_2_dim = 32\n",
    "output_dim = len(y[0])\n",
    "\n",
    "layer_1 = DNI(input_dim,layer_1_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_2 = DNI(layer_1_dim,layer_2_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_3 = DNI(layer_2_dim, output_dim,sigmoid, sigmoid_out2deriv,alpha)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "    synthetic_error = 0\n",
    "    \n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]  \n",
    "        \n",
    "        _, layer_1_out = layer_1.forward_and_synthetic_update(batch_x)\n",
    "        layer_1_delta, layer_2_out = layer_2.forward_and_synthetic_update(layer_1_out)\n",
    "        layer_3_out = layer_3.forward_and_synthetic_update(layer_2_out,False)\n",
    "\n",
    "        layer_3_delta = layer_3_out - batch_y\n",
    "        layer_2_delta = layer_3.normal_update(layer_3_delta)\n",
    "        layer_2.update_synthetic_weights(layer_2_delta)\n",
    "        layer_1.update_synthetic_weights(layer_1_delta)\n",
    "        \n",
    "        error += (np.sum(np.abs(layer_3_delta)))\n",
    "        synthetic_error += (np.sum(np.abs(layer_2_delta - layer_2.synthetic_gradient)))\n",
    "    if(iter % 100 == 99):\n",
    "        sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error) + \" Synthetic Loss:\" + str(synthetic_error))\n",
    "    if(iter % 10000 == 9999):\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
